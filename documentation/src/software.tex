\section{General}\label{sec:general}
The software has the task to gather the image information from the camera, detecting the ball, predicting where it will go in the future and controlling the motors to either stop or shoot the ball.
This task can be split into the following elements:
\begin{itemize}
    \item \textbf{Optics}: Correcting for lens distortion and converting the pixel coordinates to real world coordinates
    \item \textbf{Ball detection}: Detecting the ball in the image
    \item \textbf{Prediction}: Predicting the ball movement
    \item \textbf{Controlling the motors}: Moving the motors to the correct position
\end{itemize}
These elements are illustrated in the flowchart, shown in figure~\ref{fig:flowchart_software}
%A flowchart of the general process is illustrated in figure~\ref{fig:flowchart_software}.
% camera - undistortion - ball detection - prediction where the ball will be (y coordinate of the ball when it reaches the player) - target player position -|
%                                        |-predict when the ball will reach the player - shoot the ball                                                     -- Arduino
%                                        |-live feed to gui for debugging
%

\begin{figure}[H]
    \centering
    \begin{center}
        \begin{tikzpicture}[node distance=2cm]
            \node (in1) [io] {Camera};
            \node (pro1) [process, below of=in1] {Undistortion};
            \node (pro2) [process, below of=pro1] {Ball detection};
            \node (pro3) [process, below of=pro2] {Prediction};
            \node (pro4) [process, below of=pro3, right of=pro3, xshift=1.5cm] {Location};
            \node (pro5) [process, below of=pro3, left of=pro3, xshift=-1.5cm] {Timing};
            \node (out1) [io, below of=pro5, right of=pro5, xshift=1.5cm] {Arduino};

            % arrows
            \draw [arrow] (in1) -- (pro1);
            \draw [arrow] (pro1) -- (pro2);
            \draw [arrow] (pro2) -- (pro3);
            \draw [arrow] (pro3) |- (pro4);
            \draw [arrow] (pro3) |- (pro5);
            \draw [arrow] (pro4) |- (out1) node[below,midway,xshift=0.5cm] {Target player position};
            \draw [arrow] (pro5) |- (out1) node[below,midway] {Shoot the ball};

%%        make box around pro1 and pro2 as they are on the computer
%        \draw[red,thick] ($(pro1.north west)+(-1.3,0.6)$)  rectangle ($(pro2.south east)+(0.3,-0.3)$) node[right,midway, xshift=3cm] {Software (Chapter~\ref{ch:software})};
%%       make box from pro6 to out1
%        \draw[red,thick] ($(pro6.north west)+(-1,2.3)$)  rectangle ($(out1.south east)+(2.3,-0.3)$) node[below,midway, yshift=-4cm] {Electronics (Chapter~\ref{ch:electronics})};
        \end{tikzpicture}
    \end{center}
    \caption[flowchart]{General flowchart}
    \label{fig:flowchart_software}
\end{figure}
The whole flowchart can be seen in the appendix~\ref{sec:whole-flowchart}.
%https://github.com/TomtheCoder2/Maturitaetsarbeit
The code can be found on GitHub\footnote{\url{https://github.com/TomtheCoder2/Maturitaetsarbeit}}.


\section{Optics}\label{sec:optics}
The goal here is to correct for lens distortion, this can be achieved by capturing many images containing a checkerboard pattern with known grid size, in this is case 8x8 and then letting an algorithm compute the distortion coefficients and the camera matrix.
One such image is shown in figure~\ref{fig:calibration_image}.
A sever is distortion is visible in the image, as the table is actually a rectangle, this can be corrected by using the OpenCV library.
\begin{figure}[H]
    \centering
    \includegraphics[height=5cm]{../photos/calibration_image}
    \caption[calimage]{Calibration image}
    \label{fig:calibration_image}
\end{figure}

% todo: should i explain in detail how opencv does it?
% lets do it and later i can delete it if its too much
%\todo{Either add lables for the variables or remove this part}

\subsection{OpenCV}\label{subsec:opencv}
The task of the openCV library is to correct the distortion of the image.
OpenCV is a library that provides many functions for image processing.
One of these functions is the camera calibration function.
The function takes the images of the checkerboard pattern and calculates the distortion coefficients and the camera matrix.
The key principal is to map the distorted image to an undistorted image
This is done by calculating the pixel coordinates of the undistorted image for each pixel in the distorted image.
The formulas for the radial and tangential distortion are as follows:\autocite{opencv-undistort}
\begin{itemize}
    \item \textbf{Radial distortion:} This is the distortion that makes straight lines appear curved:
    \begin{gather*}
        r^2 = x^2 + y^2\\
        x_{\text{radial}} = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\\
        y_{\text{radial}} = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\\,
    \end{gather*}
    where \(x,y\) are the normalized pixel coordinates in the undistorted image and \( k_1, k_2, k_3 \) are the radial distortion coefficients.
    An example of radial distortion is depicted in figure~\ref{fig:radial_distortion}.
    The red rectangle shows how the table should look like.
    \begin{figure}[H]
%        \centering
%        \begin{subfigure}[!htb]{0.5\textwidth}
        \centering
        \begin{tikzpicture}% based on https://tex.stackexchange.com/a/9561/ (Caramdir's fantastic answer to another question)
            \node (tiger) [anchor=north west, inner sep=0pt] {\includegraphics[width=0.5\textwidth]{../photos/radial_distortion}};
            \begin{scope}
                [x={(tiger.north east)},y={(tiger.south west)}]
%                    \foreach \i/\j in {{(0.23,1.05)/(0.25,-.1)},{(0.54,1.1)/(0.5,-.15)},{(0.76,1.05)/(0.8,-0.1)}}
%                    \draw [red, thick] \i -- \j;
%                    big rect: 28, 89 - 25, 479 - 698, 495 - 713, 101
%small rect: 281, 219 - 272, 375 - 432, 386 - 440, 228
%
%dimensions: 728, 544
                \draw [red, line width=1.5mm] (0.0384615385, 0.1636029412) -- (0.0343406593, 0.8805147059) -- (698/728, 495/544) -- (713/728, 101/544) -- cycle;
%                \draw [red, line width=1.5mm] (281/728, 219/544) -- (272/728, 375/544) -- (432/728, 386/544) -- (440/728, 228/544) -- cycle;
            \end{scope}
        \end{tikzpicture}
        \caption{Radial distortion}
        \label{fig:radial_distortion}
%        \end{subfigure}
    \end{figure}


    \item \textbf{Tangential distortion:} This distortion occurs because the lens and the image sensor are not perfectly parallel.
    \begin{gather*}
        x_{\text{tangential}} = 2p_1 xy + p_2(r^2 + 2x^2)\\
        y_{\text{tangential}} = p_1(r^2 + 2y^2) + 2p_2 xy\\,
    \end{gather*}
    where \( p_1, p_2 \) are the tangential distortion coefficients.
\end{itemize}
Together that yields:
\begin{gather*}
    x' = x_{\text{radial}} + x_{\text{tangential}}\\
    y' = y_{\text{radial}} + y_{\text{tangential}}\\
\end{gather*}
Finally, the undistorted pixel positions are transformed back to image coordinates using the camera matrix \( K \):
\[
    \begin{bmatrix}
        u \\
        v
    \end{bmatrix}
    =
    K \cdot
    \begin{bmatrix}
        x' \\
        y' \\
        1
    \end{bmatrix},
\]
where $K$ is the camera matrix:
\begin{center}
    K = \begin{bmatrix}
            f_x & 0   & c_x \\
            0   & f_y & c_y \\
            0   & 0   & 1
    \end{bmatrix}\\,
\end{center}
where:
\begin{itemize}
    \item $f_x$ and $f_y$ are the focal lengths along the x and y axes (in pixels),
    \item $c_x$ and $c_y$ are the coordinates of the optical center (principal point), typically at the center of the image.
\end{itemize}
These formulas allow the distorted image points to be remapped to undistorted coordinates.\autocite{opencv-undistort}

\subsubsection{Implementation}\label{subsubsec:implementation}
Using the undistort function provided by openCV\autocite{opencv-undistort} is not fast enough for our needs, because the program only has 3ms to do the whole image processing and motor controlling.
So I wrote a custom function that generates a table where the index for each pixel in the new image is stored, so the function does not have to calculate the pixel coordinate for the undistorted image each time when the function is called.
The rust function to get the coordinates $x_{\text{original}},y_{\text{original}}$ of a pixel at $x_{\text{undistorted}},y_{\text{undistorted}}$ in the distorted (original) image looks like this:


%\todo{Should i move these code snippets into the appendix?}
\begin{lstlisting}[language=rust,breaklines,label={lst:distort_coords}]
/// Calculate the distorted coordinates given undistorted image coordinates.
fn distort_coords(x: f64, y: f64, fx: f64, fy: f64, cx: f64, cy: f64) -> (f64, f64) {
    // Distortion coefficients
    let k1 = DIST_COEFFS[0];
    let k2 = DIST_COEFFS[1];
    let p1 = DIST_COEFFS[2];
    let p2 = DIST_COEFFS[3];
    let k3 = DIST_COEFFS[4];

    // Normalize coordinates to [-1, 1]
    let x_normalized = (x - cx) / fx;
    let y_normalized = (y - cy) / fy;

    // Calculate radial distance
    let r2 = x_normalized * x_normalized + y_normalized * y_normalized;
    let r4 = r2 * r2;

    // Apply radial distortion
    let radial_distortion = 1.0 + k1 * r2 + k2 * r4 + k3 * r4 * r2;
    let x_radial = x_normalized * radial_distortion;
    let y_radial = y_normalized * radial_distortion;

    // Apply tangential distortion
    let x_tangential =
        2.0 * p1 * x_normalized * y_normalized + p2 * (r2 + 2.0 * x_normalized * x_normalized);
    let y_tangential =
        p1 * (r2 + 2.0 * y_normalized * y_normalized) + 2.0 * p2 * x_normalized * y_normalized;

    // Distorted coordinates
    let x_distorted = x_radial + x_tangential;
    let y_distorted = y_radial + y_tangential;

    // Map back to pixel coordinates
    let distorted_x = fx * x_distorted + cx;
    let distorted_y = fy * y_distorted + cy;

    (distorted_x, distorted_y)
}
\end{lstlisting}
Where \texttt{DIST\_COEFFS} are the distortion coefficients calculated by the OpenCV calibration function.
To generate the table, I created the following code:
\begin{lstlisting}[language=rust,breaklines,label={lst:gen_table}]
/// Generate precomputation table for undistortion.
pub fn gen_table(
    original_width: u32, original_height: u32,
    new_width: u32, new_height: u32,
    x_offset: i32, y_offset: i32,
) -> Vec<i32> {
    // Camera matrix values
    let fx = CAMERA_MATRIX[0][0];
    let fy = CAMERA_MATRIX[1][1];
    let cx = CAMERA_MATRIX[0][2];
    let cy = CAMERA_MATRIX[1][2];
    let mut precomputation_table = vec![];

    for y in 0..new_height {
        for x in 0..new_width {
            let x = x as i32 + x_offset;
            let y = y as i32 + y_offset;
            // Map the pixel back to the distorted image coordinates
            let (distorted_x, distorted_y) = distort_coords(x as f64, y as f64, fx, fy, cx, cy);

            let distorted_x = distorted_x.round() as i32;
            let distorted_y = distorted_y.round() as i32;

            // If the coordinates are within the bounds of the original image, map the pixel
            let index = if distorted_x >= 0
                && distorted_x < original_width as i32
                && distorted_y >= 0
                && distorted_y < original_height as i32
            {
                let index = ((distorted_y * original_width as i32 + distorted_x) * 3) as usize;
                index as i32
            } else {
                -1 // black pixel (outside the image bounds)
            };
            precomputation_table.push(index);
        }
    }
    precomputation_table
}
\end{lstlisting}
This generates a long vector with the corresponding index for each pixel in the undistorted image.
(Note that these are indices because the image is flattened to a vector of length $3 \cdot \text{width} \cdot \text{height}$, where each pixel has 3 values for the RGB channels.)

Using the table is as simple as just looking up the index for the pixel in the undistorted image and copying the pixel values from the original image to the new image.
This function is called for each frame, and the result is a corrected image with no distortion.
The code can be seen here:
\begin{lstlisting}[language=rust,breaklines,label={lst:undistort_image_table}]
/// Undistort an image using the precomputed table.
pub fn undistort_image_table(
    img: &[u8],
    undistorted_img: &mut [u8],
    table: &Vec<i32>,
    new_width: u32,
    new_height: u32,
) {
    // Assert that the image has the correct size
    assert_eq!(
        undistorted_img.len(),
        3 * new_width as usize * new_height as usize
    );

    for i in 0..new_height * new_width {
        let index = table[i as usize];

        if index != -1 {
            #[inline]
            /// Helper function to avoid code duplication
            fn set_pixel(undistorted_img: &mut [u8], img: &[u8], pixel_index: usize, i: usize) {
                undistorted_img[i as usize] = img[pixel_index];
            }
            let pixel_index = index as usize;
            set_pixel(undistorted_img, img, pixel_index, i as usize * 3);
            set_pixel(undistorted_img, img, pixel_index + 1, i as usize * 3 + 1);
            set_pixel(undistorted_img, img, pixel_index + 2, i as usize * 3 + 2);
        }
    }
}
\end{lstlisting}
The parameters for this function are a bit special because one argument is the final vector where the undistorted image is stored, the other is the original image, and the last one is the table that was generated before.
Giving the final image buffer as a parameter saves time, because the image buffer can be reused in each frame, saving the time to reallocate the buffer each frame.
\\
To illustrate the effect of the undistortion, an example of a distorted and undistorted rainbow image is shown in figure~\ref{fig:original_undistorted_rainbow}.
% todo: pick a better example image
\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../photos/original_rainbow}
        \caption[originalRainbow]{Original rainbow image}
        \label{fig:original_rainbow}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../photos/undistorted_rainbow}
        \caption[originalRainbow]{Undistorted rainbow image}
        \label{fig:undistorted_rainbow}
    \end{subfigure}
    \caption{Rainbow image}
    \label{fig:original_undistorted_rainbow}
\end{figure}
The undistorted image (figure~\ref{fig:undistorted_rainbow}) is larger than the original rainbow image (figure~\ref{fig:original_rainbow}), that is because some pixels are moved out of the original image bounds because the camera can see "further" at the corners than at the sides.
This effect is also shown in the figure~\ref{fig:original_undistorted_example} where the original image~\ref{fig:original_example8} is stretched at the corners for the undistorted image~\ref{fig:undistorted_example8}.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../photos/original_example12}
        \caption[originalRainbow]{Original image}
        \label{fig:original_example8}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../photos/output12}
        \caption[originalRainbow]{Undistorted image}
        \label{fig:undistorted_example8}
    \end{subfigure}
    \caption{Example image}
    \label{fig:original_undistorted_example}
\end{figure}
Therefore I have to crop the image at the sides by a different amount, I made a simple graphical user interface (GUI) to adjust the cropping values.
Figure~\ref{fig:margin_adj_gui} illustrates the GUI to adjust the margins.
On the top there are four inputs for numbers that represent the margins at the top, bottom, left and right, which can be adjusted until the image is cropped correctly.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../photos/margin_adj_gui}
    \caption[marginadjgui]{Margin Adjustment GUI}
    \label{fig:margin_adj_gui}
\end{figure}
The GUI is also written in rust using the egui\autocite{egui} library.
%\todo{Fix the egui cite command (numbers instead of name)}
The cropped example image is shown in figure~\ref{fig:example8_cropped}.
The image is cropped so only the table is visible.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../photos/output12_cropped}
    \caption[croppedexampleimage]{Cropped example image}
    \label{fig:example8_cropped}
\end{figure}

%\todo{Move parts of this chapter into appendix}

\subsection{Real world coordinates}\label{subsec:real-world-coordinates}
An important part of the software is to convert the pixel coordinates to real-world coordinates.
This is done by adding fiducials to the table.
Fiducials are simple patterns, placed at known positions and can be detected in the image.
The fiducials are placed at the corners of the table, and the coordinates are measured in millimeters.
Figure~\ref{fig:fiducials_all} shows the fiducials.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.2\textwidth}
        \centering
        \fbox{\includegraphics[width=.7\textwidth]{../photos/fiducials/fiducial_1}}
        \caption[originalRainbow]{Fiducial 1}
        \label{fig:fid_1}
    \end{subfigure}%
    \begin{subfigure}{.2\textwidth}
        \centering
        \fbox{\includegraphics[width=.7\textwidth]{../photos/fiducials/fiducial_2}}
        \caption[originalRainbow]{Fiducial 2}
        \label{fig:fid_2}
    \end{subfigure}
    \begin{subfigure}{.2\textwidth}
        \centering
        \fbox{\includegraphics[width=.7\textwidth]{../photos/fiducials/fiducial_3}}
        \caption[originalRainbow]{Fiducial 3}
        \label{fig:fid_3}
    \end{subfigure}
    \begin{subfigure}{.2\textwidth}
        \centering
        \fbox{\includegraphics[width=.7\textwidth]{../photos/fiducials/fiducial_4}}
        \caption[originalRainbow]{Fiducial 4}
        \label{fig:fid_4}
    \end{subfigure}
    \caption{Fiducials}
    \label{fig:fiducials_all}
\end{figure}
To calibrate the real world coordinates one has to detect the fiducials reliably in the image.
This can be done by using a convolutional neural network (CNN)\autocite{cnn} to detect the fiducials.

\subsubsection{Training data}\label{subsubsec:training-data}
To train the CNN, one needs a lot of images containing the fiducials, making such a lot of images and then checking where exactly the coordinates of the midpoint of the fiducial lies is very tedious and not practical.
Therefore I wrote a Java program to generate about 250 labeled images to train the CNN.
An image containing all the fiducials is shown in figure~\ref{fig:fiducials}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{../photos/training_whole_general_image}
    \caption[fiducials]{Training fiducials}
    \label{fig:fiducials}
\end{figure}
The trick I am using is that I know that the fiducials always lie in the corners of the table, so I can generate the images by just placing the fiducials in the corners with some random offset.
The different parts of the image are shown in figure~\ref{fig:fiducials_parts}.
The red lines divide the image in four horizontal and six vertical parts.
The program only looks at the corners of the table, as the fiducials are always in the corners and therefore the CNN does not have to scan the whole image.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Include the image
        \node[anchor=south west, inner sep=0] (image) at (0,0) {\includegraphics[width=0.5\textwidth]{../photos/base_img}};
        \begin{scope}
            [x={(image.south east)}, y={(image.north west)}]
            % vertical lines (6 parts)
            \foreach \i in {1/6, 2/6, 3/6, 4/6, 5/6} {
                \draw[red, very thick] (\i, 0) -- (\i, 1);
            }
            % horizontall lines (4 parts)
            \foreach \i in {1/4, 2/4, 3/4} {
                \draw[red, very thick] (0, \i) -- (1, \i);
            }
        \end{scope}
    \end{tikzpicture}
    \caption{Image split into 4 horizontal and 6 vertical parts}
    \label{fig:fiducials_parts}
\end{figure}
\textbf{Create and Train the CNN}\\
I made two separate CNNs, one for detecting the fiducial (number from 1-4 as there are four fiducials) (also known as classification) and one for detecting the coordinate of the midpoint of the fiducial (from where the real-world measurements are made).
The CNNs are trained using the tensorflow\autocite{tf} library made by google.
The structure of the CNNs is shown in the section~\ref{subsec:cnn-architecture} in the appendix.

To visualize the way the CNN works I use Grad-CAM\autocite{Selvaraju_2019} to visualize the gradient of the image, which corresponds to the parts of the image the CNN is looking at.
Grad-Cam is an algorithm to create a saliency map for the CNN, this is illustrated in figure~\ref{fig:gradcam_combined_image}.
There is always first the name of the layer as a title then the heat map alone on the left and on the right there is an overlay of the original image and the heat map.
And indeed, we see that in the last few layers, the CNN focuses on the fiducial and the corners of the table, which is quite fascinating to see, as it mimics human behavior.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../photos/gradcam_combined_image_cropped}
    \caption[cnn-gradcam]{Grad-CAM of the last few layers (using another CNN which was trained to do the same task). Figure~\ref{fig:gradcam} in the appendix shows all the layers}
    \label{fig:gradcam_combined_image}
\end{figure}\\
%\todo{short description where the CNN is looking at, why it is cool}

% Todo add stats about the real world training/tests
%\todo{Todo add stats about the real world training/tests}


\section{Ball detection}\label{sec:ball-detection}
The ball detection is a crucial part of the software, because if the ball is not detected correctly, the whole system will not work.
To detect the ball an image is taken without the ball present (called raw image) and then the image with the ball is subtracted from the image without the ball, so that only the ball remains in the image.
However a problem that arises is that also other things that change are detected, like people moving or the table moving.
Luckily the glass table is very reflective and the light from the bottom is strong enough that the people behind the glass table have a low contrast in the image, so they are not detected.
To detect the ball I just filter out the white pixels in the image, because the ball is white.
The ball detection in action is shown in figure~\ref{fig:ball_detection}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../photos/ball_detection}
    \caption[ball-detection]{Ball detection in action}
    \label{fig:ball_detection}
\end{figure}
But having the pixel coordinates is not very useful, so the program has to convert the pixel coordinates to real-world coordinates.
To compute the real world coordinates, the program compares the pixel coordinates of the ball to the pixel coordinates of the fiducials.
The program needs the real world coordinates to move the motors to the correct position as it is possible that the camera moves or the table moves, so the pixel coordinates are not reliable.
I measured the distance between the walls and the fiducials and made a coordinate system so that the fiducial with the number 2 lies in the top left corner (0,0) and the other fiducials are placed accordingly.
Then, to compute the real world coordinates, the program uses a homography\autocite{homography} matrix to transform the pixel coordinates to real world coordinates.


\section{Homography Calculation}\label{sec:homography-calculation}

Homography is used to map points from image coordinates to real-world coordinates.
The transformation is defined as:

\begin{equation}
    \begin{bmatrix}
        X' \\
        y' \\
        1
    \end{bmatrix}
    = H \cdot
    \begin{bmatrix}
        x \\
        y \\
        1
    \end{bmatrix},
    \label{eq:homography}
\end{equation}

where:
\begin{itemize}
    \item \( (x, y) \) are the pixel coordinates,
    \item \( (x', y') \) are the real-world coordinates,
    \item \( H \) is the \( 3 \times 3 \) homography matrix.
\end{itemize}

In Python, computing \( H \) requires the $cv2.findHomography$ function:

%\begin{lstlisting}[language=python,breaklines,label={lst:python_homography}]
%homography_matrix, _ = cv2.findHomography(pixel_points, real_points)
%\end{lstlisting}

\[
    H, \text{\_} = \text{cv2.findHomography}(\text{pixel\_points}, \text{real\_points})
\]
The homography matrix can be calculated using some basic linear algebra that can be found here~\autocite{homography}.

In Rust, this matrix is applied to new pixel points via matrix multiplication and normalization.
% todo: should i explain that i save the stuff to a file and then read it to save time?


\section{Prediction}\label{sec:prediction}
To predict where the ball will go in the future, the program uses a simple linear regression\autocite{linear_regression} model.
But first, the program has to check whether the ball is still rolling into the same direction and with the same velocity.
This is done by using the last position and computing the current position with the current velocity, which was calculated by the last two positions.
If the difference between the prediction and the real world position is too large, then the program deletes the positions and start over.
The prediction of the future position of the ball is shown in figure~\ref{fig:ball_prediction}.
The background is black, because in this image the raw image is subtracted from the image with the ball, so only the ball remains.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../photos/ball_prediction}
    \caption[ball-prediction]{Prediction of the future positions of the ball.
    The green balls are the most recent positions and the violet line is the current velocity direction}
    \label{fig:ball_prediction}
\end{figure}
% todo: should i explain the linear regression model and the linear algebra behind it?
%\todo{Should i explain the linear regression model and the linear algebra behind it?}
To find out where the ball will end up when it reaches the goalkeeper (or any player), the program can just compute the intersection between the line of the ball and the line of the goalkeeper.
The line of the ball can be computed with the current position $\vec{r}$, the current velocity $\vec{v}$ and the player's $p_x$ coordinate.
\begin{equation}
    \begin{split}
        \vec{r} &= \begin{bmatrix}
                       r_x \\
                       r_y
        \end{bmatrix}\\
        \vec{v} &= \begin{bmatrix}
                       v_x \\
                       v_y
        \end{bmatrix}\\
%    let t = (x - self.position.x) / self.velocity.x;
%        Some(self.position + self.velocity * t)
        t &= \frac{p_x - r_x}{v_x}\\
        \vec{r}_\text{intersection} &= \vec{r} + \vec{v} \cdot t
    \end{split}\label{eq:ball_intersection}
\end{equation}
Now the program only has to convert the $\vec{r}_\text{intersection}$ to real world coordinates and gets the position where the ball will end up, which the program sends to the arduino to move the goalkeeper to this position.
Figure~\ref{fig:ball-prediction} shows the prediction of the ball movement.
In this case the raw image subtracted from the image but only for the ball detection algorithm and then added back to the image.
The yellow circle shows where the ball will intersect with the player.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../photos/ball_prediction2}
    \caption[ball-detection]{Prediction of the ball movement. The yellow circle shows where the ball will intersect with the player}
    \label{fig:ball-prediction}
\end{figure}

%
%\todo{Can be removed, and link to electronics chapter, correct?}
%\section{Controlling the motors}\label{sec:controlling-the-motors}
%To control the motors I use an Arduino Mega 2560, because it has enough pins to control all the motors at once.
%The Arduino is connected to the computer via USB, and the computer sends the motor positions to the Arduino via serial communication.
%
%\subsubsection{Controlling the DC-Motor}\label{subsubsec:controlling-the-dc-motor}
%The DC-Motor is controlled by the stepper motor driver L298N as seen in Chapter~\ref{ch:electronics}.
%



